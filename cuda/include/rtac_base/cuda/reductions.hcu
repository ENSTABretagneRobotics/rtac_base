#ifndef _RTAC_BASE_CUDA_REDUCTIONS_HCU_
#define _RTAC_BASE_CUDA_REDUCTIONS_HCU_

#include <rtac_base/cuda/operators.h>

namespace rtac { namespace cuda { namespace device {

// From NVIDIA reduction tutorial by Mark Harris
template <typename T, unsigned int BlockSize, template<typename> class OperatorT>
__device__ void warp_reduce(volatile T* sdata, unsigned int tid)
{
    if (BlockSize >= 64) OperatorT<volatile T>::apply(sdata[tid], sdata[tid + 32]);
    if (BlockSize >= 32) OperatorT<volatile T>::apply(sdata[tid], sdata[tid + 16]);
    if (BlockSize >= 16) OperatorT<volatile T>::apply(sdata[tid], sdata[tid +  8]);
    if (BlockSize >=  8) OperatorT<volatile T>::apply(sdata[tid], sdata[tid +  4]);
    if (BlockSize >=  4) OperatorT<volatile T>::apply(sdata[tid], sdata[tid +  2]);
    if (BlockSize >=  2) OperatorT<volatile T>::apply(sdata[tid], sdata[tid +  1]);
}

template <typename T, unsigned int BlockSize, template<typename> class OperatorT>
__device__ void reduce_shared(volatile T* sdata, unsigned int tid)
{
    if (BlockSize >= 1024) { if(tid < 512) { 
        OperatorT<volatile T>::apply(sdata[tid], sdata[tid + 512]); __syncthreads();
    }}
    if (BlockSize >= 512) { if(tid < 256) { 
        OperatorT<volatile T>::apply(sdata[tid], sdata[tid + 256]); __syncthreads();
    }}
    if (BlockSize >= 256) { if(tid < 128) { 
        OperatorT<volatile T>::apply(sdata[tid], sdata[tid + 128]); __syncthreads();
    }}
    if (BlockSize >= 128) { if(tid <  64) { 
        OperatorT<volatile T>::apply(sdata[tid], sdata[tid +  64]); __syncthreads();
    }}

    if(tid < 32) warp_reduce<T,BlockSize,OperatorT>(sdata, tid);
}

template <typename T, unsigned int BlockSize = 256,
          template<typename> class OperatorT = Addition>
__global__ void do_reduce(const T* in, T* out, unsigned int N,
        unsigned int inputLineStride = 0, unsigned int outputLineStride = 0)
{
    // Shared data must be declared this way because NVCC does not accept
    // shared data declarations with same names but different types even if
    // they will never be in the same scope. (will happen on template
    // instanciations with different T).
    // extern __shared__ T sdata[]; // WRONG
    //extern __shared__ __align__(sizeof(T)) unsigned char sdata_[];
    extern __shared__ unsigned char sdata_[];
    T *sdata = reinterpret_cast<T*>(sdata_);

    unsigned int tid = threadIdx.x;

    unsigned int i = BlockSize*blockIdx.x + tid;
    unsigned int gridSize = BlockSize*gridDim.x;

    sdata[tid] = OperatorT<T>::Neutral;
    while(i < N) {
        OperatorT<T>::apply(sdata[tid], in[i + inputLineStride*blockIdx.y]);
        i += gridSize;
    }
    __syncthreads();
    
    reduce_shared<T, BlockSize, OperatorT>(sdata, tid);

    if(tid == 0) out[blockIdx.x + outputLineStride*blockIdx.y] = sdata[0];
}

template <typename T, unsigned int BlockSize = 256,
          template<typename> class OperatorT = Addition>
void reduce(T* in, T* out, unsigned int N, cudaStream_t stream = 0)
{
    while(N > 0) {
        unsigned int blockCount = N / (2*BlockSize);
        //unsigned int blockCount = N / BlockSize;
        if(blockCount == 0) {
            do_reduce<T,BlockSize,OperatorT>
            <<<1, BlockSize, sizeof(T)*BlockSize, stream>>>(in, out, N);
        }
        else {
            do_reduce<T,BlockSize,OperatorT>
            <<<blockCount, BlockSize, sizeof(T)*BlockSize, stream>>>(in, in, N);
        }
        cudaStreamSynchronize(stream);
        N = blockCount;
    }
}

template <typename T, unsigned int BlockSize = 256,
          template<typename> class OperatorT = Addition>
void reduce_lines(T* in, T* out, unsigned int width, unsigned int height,
                  unsigned int inputStride  = 0,
                  unsigned int outputStride = 1,
                  cudaStream_t stream = 0)
{
    if(!inputStride) {
        inputStride = width;
    }

    unsigned int N = width;
    while(N > 0) {
        unsigned int blockCount = N / (2*BlockSize);
        //unsigned int blockCount = N / BlockSize;
        if(blockCount == 0) {
            do_reduce<T,BlockSize,OperatorT>
                <<<dim3(1,height,1), BlockSize, sizeof(T)*BlockSize, stream>>>(
                    in, out, N, inputStride, outputStride);
        }
        else {
            do_reduce<T,BlockSize,OperatorT>
                <<<dim3(blockCount,height,1), BlockSize, sizeof(T)*BlockSize, stream>>>(
                    in, in, N, inputStride, inputStride);
        }
        cudaStreamSynchronize(stream);
        N = blockCount;
    }
}

}; //namespace device
}; //namespace cuda
}; //namespace rtac

#endif //_RTAC_BASE_CUDA_REDUCTIONS_H_
